{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Optimization in Python\n",
    "\n",
    "This notebook covers performance optimization techniques for Python code, focusing on offline development scenarios. Learn how to identify bottlenecks, optimize algorithms, and improve code efficiency without external tools.\n",
    "\n",
    "## What you'll learn:\n",
    "- Performance measurement techniques\n",
    "- Algorithm optimization\n",
    "- Memory optimization\n",
    "- Profiling and benchmarking\n",
    "- Common performance pitfalls\n",
    "- Optimization best practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Measuring Performance\n",
    "\n",
    "### Using time module for basic timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import List\n",
    "\n",
    "# Example: Measuring execution time\n",
    "def measure_time(func, *args, **kwargs):\n",
    "    \"\"\"Measure the execution time of a function.\"\"\"\n",
    "    start_time = time.time()\n",
    "    result = func(*args, **kwargs)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    return result, execution_time\n",
    "\n",
    "# Test function\n",
    "def sum_squares(n: int) -> int:\n",
    "    \"\"\"Calculate sum of squares from 1 to n.\"\"\"\n",
    "    return sum(i**2 for i in range(1, n + 1))\n",
    "\n",
    "# Measure performance\n",
    "result, exec_time = measure_time(sum_squares, 10000)\n",
    "print(f\"Result: {result}\")\n",
    "print(f\"Execution time: {exec_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing different implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different implementations of the same algorithm\n",
    "def sum_squares_loop(n: int) -> int:\n",
    "    \"\"\"Using explicit loop.\"\"\"\n",
    "    total = 0\n",
    "    for i in range(1, n + 1):\n",
    "        total += i ** 2\n",
    "    return total\n",
    "\n",
    "def sum_squares_comprehension(n: int) -> int:\n",
    "    \"\"\"Using list comprehension.\"\"\"\n",
    "    return sum(i**2 for i in range(1, n + 1))\n",
    "\n",
    "def sum_squares_math(n: int) -> int:\n",
    "    \"\"\"Using mathematical formula: n(n+1)(2n+1)/6\"\"\"\n",
    "    return n * (n + 1) * (2 * n + 1) // 6\n",
    "\n",
    "# Compare performance\n",
    "n = 100000\n",
    "\n",
    "print(\"Comparing different implementations:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test loop implementation\n",
    "result1, time1 = measure_time(sum_squares_loop, n)\n",
    "print(f\"Loop:           {time1:.6f}s (result: {result1})\")\n",
    "\n",
    "# Test comprehension implementation\n",
    "result2, time2 = measure_time(sum_squares_comprehension, n)\n",
    "print(f\"Comprehension:  {time2:.6f}s (result: {result2})\")\n",
    "\n",
    "# Test mathematical implementation\n",
    "result3, time3 = measure_time(sum_squares_math, n)\n",
    "print(f\"Mathematical:   {time3:.6f}s (result: {result3})\")\n",
    "\n",
    "# Verify results are the same\n",
    "assert result1 == result2 == result3\n",
    "print(f\"\\nâœ… All results match! Mathematical approach is {time1/time3:.1f}x faster.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Algorithm Optimization\n",
    "\n",
    "### Example: Finding duplicates in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inefficient approach\n",
    "def find_duplicates_slow(nums: List[int]) -> List[int]:\n",
    "    \"\"\"Find all duplicate elements using nested loops.\"\"\"\n",
    "    duplicates = []\n",
    "    seen = set()\n",
    "    \n",
    "    for i in range(len(nums)):\n",
    "        if nums[i] in seen:\n",
    "            if nums[i] not in duplicates:\n",
    "                duplicates.append(nums[i])\n",
    "        else:\n",
    "            for j in range(i + 1, len(nums)):\n",
    "                if nums[i] == nums[j]:\n",
    "                    duplicates.append(nums[i])\n",
    "                    break\n",
    "            seen.add(nums[i])\n",
    "    \n",
    "    return duplicates\n",
    "\n",
    "# Efficient approach\n",
    "def find_duplicates_fast(nums: List[int]) -> List[int]:\n",
    "    \"\"\"Find all duplicate elements using hash set.\"\"\"\n",
    "    seen = set()\n",
    "    duplicates = set()\n",
    "    \n",
    "    for num in nums:\n",
    "        if num in seen:\n",
    "            duplicates.add(num)\n",
    "        else:\n",
    "            seen.add(num)\n",
    "    \n",
    "    return list(duplicates)\n",
    "\n",
    "# Test data\n",
    "import random\n",
    "test_data = [random.randint(1, 1000) for _ in range(5000)]\n",
    "test_data.extend(test_data[:100])  # Add some duplicates\n",
    "\n",
    "print(\"Finding duplicates in a list of 5000+ elements:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test slow approach\n",
    "result1, time1 = measure_time(find_duplicates_slow, test_data)\n",
    "print(f\"Slow approach: {time1:.6f}s (found {len(result1)} duplicates)\")\n",
    "\n",
    "# Test fast approach\n",
    "result2, time2 = measure_time(find_duplicates_fast, test_data)\n",
    "print(f\"Fast approach: {time2:.6f}s (found {len(result2)} duplicates)\")\n",
    "\n",
    "# Verify results\n",
    "assert set(result1) == set(result2)\n",
    "print(f\"\\nâœ… Results match! Fast approach is {time1/time2:.1f}x faster.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big O Notation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing time complexity\n",
    "def analyze_complexity():\n",
    "    \"\"\"Demonstrate different time complexities.\"\"\"\n",
    "    import math\n",
    "    \n",
    "    sizes = [100, 1000, 10000]\n",
    "    \n",
    "    print(\"Time Complexity Analysis:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(\"n\\tO(1)\\tO(log n)\\tO(n)\\tO(n log n)\\tO(nÂ²)\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for n in sizes:\n",
    "        o1 = 1\n",
    "        ologn = math.log2(n)\n",
    "        on = n\n",
    "        onlogn = n * math.log2(n)\n",
    "        on2 = n * n\n",
    "        \n",
    "        print(f\"{n}\\t{o1}\\t{ologn:.1f}\\t{on}\\t{onlogn:.0f}\\t\\t{on2}\")\n",
    "    \n",
    "    print(\"\\nKey insights:\")\n",
    "    print(\"- O(1): Constant time - best performance\")\n",
    "    print(\"- O(log n): Logarithmic - excellent for large datasets\")\n",
    "    print(\"- O(n): Linear - good performance\")\n",
    "    print(\"- O(n log n): Acceptable for most applications\")\n",
    "    print(\"- O(nÂ²): Quadratic - avoid for large datasets\")\n",
    "\n",
    "analyze_complexity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Memory Optimization\n",
    "\n",
    "### Efficient data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from typing import Dict, List\n",
    "\n",
    "# Compare memory usage of different approaches\n",
    "def memory_comparison():\n",
    "    \"\"\"Compare memory usage of different data structures.\"\"\"\n",
    "    \n",
    "    # Create test data\n",
    "    data = list(range(10000))\n",
    "    \n",
    "    # List approach\n",
    "    list_memory = sys.getsizeof(data)\n",
    "    \n",
    "    # Set approach (for membership testing)\n",
    "    set_data = set(data)\n",
    "    set_memory = sys.getsizeof(set_data)\n",
    "    \n",
    "    # Dictionary approach\n",
    "    dict_data = {i: i**2 for i in data}\n",
    "    dict_memory = sys.getsizeof(dict_data)\n",
    "    \n",
    "    print(\"Memory Usage Comparison:\")\n",
    "    print(\"=\" * 30)\n",
    "    print(f\"List (10,000 ints): {list_memory} bytes\")\n",
    "    print(f\"Set (10,000 ints):  {set_memory} bytes\")\n",
    "    print(f\"Dict (10,000 pairs): {dict_memory} bytes\")\n",
    "    \n",
    "    # Test lookup performance\n",
    "    target = 5000\n",
    "    \n",
    "    # List lookup (O(n))\n",
    "    _, list_time = measure_time(lambda: target in data)\n",
    "    \n",
    "    # Set lookup (O(1))\n",
    "    _, set_time = measure_time(lambda: target in set_data)\n",
    "    \n",
    "    # Dict lookup (O(1))\n",
    "    _, dict_time = measure_time(lambda: dict_data.get(target))\n",
    "    \n",
    "    print(f\"\\nLookup Performance (target: {target}):\")\n",
    "    print(f\"List: {list_time:.8f}s\")\n",
    "    print(f\"Set:  {set_time:.8f}s ({list_time/set_time:.0f}x faster)\")\n",
    "    print(f\"Dict: {dict_time:.8f}s ({list_time/dict_time:.0f}x faster)\")\n",
    "\n",
    "memory_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator vs List Comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory-efficient processing with generators\n",
    "def process_with_list(n: int) -> List[int]:\n",
    "    \"\"\"Process data using list comprehension.\"\"\"\n",
    "    return [i**2 for i in range(n) if i % 2 == 0]\n",
    "\n",
    "def process_with_generator(n: int):\n",
    "    \"\"\"Process data using generator expression.\"\"\"\n",
    "    return (i**2 for i in range(n) if i % 2 == 0)\n",
    "\n",
    "# Compare memory usage\n",
    "n = 100000\n",
    "\n",
    "print(\"Memory Comparison: List vs Generator\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# List approach\n",
    "list_result = process_with_list(n)\n",
    "list_memory = sys.getsizeof(list_result)\n",
    "print(f\"List memory: {list_memory} bytes\")\n",
    "\n",
    "# Generator approach\n",
    "gen_result = process_with_generator(n)\n",
    "gen_memory = sys.getsizeof(gen_result)\n",
    "print(f\"Generator memory: {gen_memory} bytes\")\n",
    "\n",
    "print(f\"\\nMemory savings: {((list_memory - gen_memory) / list_memory * 100):.1f}%\")\n",
    "\n",
    "# Test processing time\n",
    "def sum_list():\n",
    "    return sum(process_with_list(n))\n",
    "\n",
    "def sum_generator():\n",
    "    return sum(process_with_generator(n))\n",
    "\n",
    "_, list_time = measure_time(sum_list)\n",
    "_, gen_time = measure_time(sum_generator)\n",
    "\n",
    "print(f\"\\nProcessing time:\")\n",
    "print(f\"List: {list_time:.4f}s\")\n",
    "print(f\"Generator: {gen_time:.4f}s\")\n",
    "print(f\"Generator is {list_time/gen_time:.1f}x faster for large datasets!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Profiling Techniques\n",
    "\n",
    "### Simple profiling with time measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile different parts of a function\n",
    "def profile_function():\n",
    "    \"\"\"A function with multiple operations to profile.\"\"\"\n",
    "    \n",
    "    # Operation 1: Data generation\n",
    "    start = time.time()\n",
    "    data = [i for i in range(10000)]\n",
    "    gen_time = time.time() - start\n",
    "    \n",
    "    # Operation 2: Processing\n",
    "    start = time.time()\n",
    "    processed = [x * 2 + 1 for x in data]\n",
    "    proc_time = time.time() - start\n",
    "    \n",
    "    # Operation 3: Filtering\n",
    "    start = time.time()\n",
    "    filtered = [x for x in processed if x % 3 == 0]\n",
    "    filter_time = time.time() - start\n",
    "    \n",
    "    # Operation 4: Aggregation\n",
    "    start = time.time()\n",
    "    result = sum(filtered)\n",
    "    agg_time = time.time() - start\n",
    "    \n",
    "    return {\n",
    "        'result': result,\n",
    "        'times': {\n",
    "            'generation': gen_time,\n",
    "            'processing': proc_time,\n",
    "            'filtering': filter_time,\n",
    "            'aggregation': agg_time,\n",
    "            'total': gen_time + proc_time + filter_time + agg_time\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Run profiling\n",
    "profile_result = profile_function()\n",
    "\n",
    "print(\"Function Profiling Results:\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Final result: {profile_result['result']}\")\n",
    "print(\"\\nExecution times:\")\n",
    "\n",
    "times = profile_result['times']\n",
    "total_time = times['total']\n",
    "\n",
    "for operation, op_time in times.items():\n",
    "    if operation != 'total':\n",
    "        percentage = (op_time / total_time) * 100\n",
    "        print(f\"  {operation.capitalize()}: {op_time:.6f}s ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTotal time: {total_time:.6f}s\")\n",
    "\n",
    "# Identify bottleneck\n",
    "bottleneck = max(times.items(), key=lambda x: x[1] if x[0] != 'total' else 0)\n",
    "print(f\"\\nðŸš¨ Bottleneck: {bottleneck[0]} ({bottleneck[1]:.6f}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Common Performance Pitfalls\n",
    "\n",
    "### String concatenation in loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inefficient string concatenation\n",
    "def build_string_slow(n: int) -> str:\n",
    "    \"\"\"Inefficient string building using + operator.\"\"\"\n",
    "    result = \"\"\n",
    "    for i in range(n):\n",
    "        result += str(i)\n",
    "    return result\n",
    "\n",
    "# Efficient string concatenation\n",
    "def build_string_fast(n: int) -> str:\n",
    "    \"\"\"Efficient string building using join.\"\"\"\n",
    "    return \"\".join(str(i) for i in range(n))\n",
    "\n",
    "# Compare performance\n",
    "n = 10000\n",
    "\n",
    "print(\"String Concatenation Performance:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "_, slow_time = measure_time(build_string_slow, n)\n",
    "_, fast_time = measure_time(build_string_fast, n)\n",
    "\n",
    "print(f\"Slow method (+): {slow_time:.6f}s\")\n",
    "print(f\"Fast method (join): {fast_time:.6f}s\")\n",
    "print(f\"\\nJoin is {slow_time/fast_time:.1f}x faster!\")\n",
    "\n",
    "# Verify results are the same\n",
    "assert build_string_slow(100) == build_string_fast(100)\n",
    "print(\"âœ… Results are identical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unnecessary computations in loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inefficient: Computing length in each iteration\n",
    "def process_list_slow(items: List[int]) -> List[int]:\n",
    "    \"\"\"Process list with length calculation in loop.\"\"\"\n",
    "    result = []\n",
    "    for i in range(len(items)):  # len() called each iteration\n",
    "        if items[i] > 0:\n",
    "            result.append(items[i] * 2)\n",
    "    return result\n",
    "\n",
    "# Efficient: Calculate length once\n",
    "def process_list_fast(items: List[int]) -> List[int]:\n",
    "    \"\"\"Process list with length calculated once.\"\"\"\n",
    "    result = []\n",
    "    n = len(items)  # Calculate once\n",
    "    for i in range(n):\n",
    "        if items[i] > 0:\n",
    "            result.append(items[i] * 2)\n",
    "    return result\n",
    "\n",
    "# Test data\n",
    "test_data = list(range(-50000, 50000))\n",
    "\n",
    "print(\"Loop Optimization:\")\n",
    "print(\"=\" * 20)\n",
    "\n",
    "_, slow_time = measure_time(process_list_slow, test_data)\n",
    "_, fast_time = measure_time(process_list_fast, test_data)\n",
    "\n",
    "print(f\"Slow (len in loop): {slow_time:.6f}s\")\n",
    "print(f\"Fast (len once):    {fast_time:.6f}s\")\n",
    "print(f\"\\nOptimization gives {slow_time/fast_time:.1f}x speedup!\")\n",
    "\n",
    "# Verify results\n",
    "assert process_list_slow(test_data) == process_list_fast(test_data)\n",
    "print(\"âœ… Results are identical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Caching and Memoization\n",
    "\n",
    "### Simple caching with dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fibonacci without caching (exponential time)\n",
    "def fib_slow(n: int) -> int:\n",
    "    \"\"\"Calculate nth Fibonacci number without caching.\"\"\"\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    return fib_slow(n - 1) + fib_slow(n - 2)\n",
    "\n",
    "# Fibonacci with caching (linear time)\n",
    "def fib_fast(n: int, cache: Dict[int, int] = None) -> int:\n",
    "    \"\"\"Calculate nth Fibonacci number with caching.\"\"\"\n",
    "    if cache is None:\n",
    "        cache = {}\n",
    "    \n",
    "    if n in cache:\n",
    "        return cache[n]\n",
    "    \n",
    "    if n <= 1:\n",
    "        cache[n] = n\n",
    "        return n\n",
    "    \n",
    "    result = fib_fast(n - 1, cache) + fib_fast(n - 2, cache)\n",
    "    cache[n] = result\n",
    "    return result\n",
    "\n",
    "# Compare performance\n",
    "n = 35\n",
    "\n",
    "print(\"Fibonacci Performance Comparison:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Test without caching (only for small n)\n",
    "if n <= 30:\n",
    "    result1, time1 = measure_time(fib_slow, n)\n",
    "    print(f\"Without caching: {time1:.6f}s (result: {result1})\")\n",
    "else:\n",
    "    print(\"Without caching: Too slow for n > 30\")\n",
    "    time1 = float('inf')\n",
    "\n",
    "# Test with caching\n",
    "result2, time2 = measure_time(fib_fast, n)\n",
    "print(f\"With caching:    {time2:.6f}s (result: {result2})\")\n",
    "\n",
    "if time1 != float('inf'):\n",
    "    print(f\"\\nCaching gives {time1/time2:.0f}x speedup!\")\n",
    "else:\n",
    "    print(\"\\nCaching makes it feasible for large n!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Best Practices\n",
    "\n",
    "### Optimization checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance optimization checklist\n",
    "def performance_checklist():\n",
    "    \"\"\"Display performance optimization checklist.\"\"\"\n",
    "    \n",
    "    checklist = {\n",
    "        \"Algorithm Selection\": [\n",
    "            \"Choose the right algorithm for the job\",\n",
    "            \"Consider time/space complexity trade-offs\",\n",
    "            \"Use built-in functions when possible\",\n",
    "            \"Avoid reinventing the wheel\"\n",
    "        ],\n",
    "        \"Data Structures\": [\n",
    "            \"Use sets for membership testing\",\n",
    "            \"Use dictionaries for fast lookups\",\n",
    "            \"Consider memory vs speed trade-offs\",\n",
    "            \"Use generators for large datasets\"\n",
    "        ],\n",
    "        \"Code Optimization\": [\n",
    "            \"Move invariant code out of loops\",\n",
    "            \"Use list comprehensions wisely\",\n",
    "            \"Avoid unnecessary function calls\",\n",
    "            \"Use appropriate data types\"\n",
    "        ],\n",
    "        \"Measurement\": [\n",
    "            \"Measure before optimizing\",\n",
    "            \"Identify bottlenecks first\",\n",
    "            \"Use appropriate profiling tools\",\n",
    "            \"Test optimizations thoroughly\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for category, items in checklist.items():\n",
    "        print(f\"\\n{category}:\")\n",
    "        for item in items:\n",
    "            print(f\"  âœ… {item}\")\n",
    "\n",
    "performance_checklist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Exercise: Optimize a Function\n",
    "\n",
    "Here's a function that can be optimized. Your task is to:\n",
    "1. Identify performance bottlenecks\n",
    "2. Optimize the algorithm\n",
    "3. Measure the improvements\n",
    "4. Ensure correctness is maintained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to optimize\n",
    "def find_common_elements_slow(lists: List[List[int]]) -> List[int]:\n",
    "    \"\"\"Find elements common to all lists (inefficient version).\"\"\"\n",
    "    if not lists:\n",
    "        return []\n",
    "    \n",
    "    # Start with first list\n",
    "    common = lists[0][:]\n",
    "    \n",
    "    # Check each element against all other lists\n",
    "    for element in common[:]:\n",
    "        for other_list in lists[1:]:\n",
    "            if element not in other_list:\n",
    "                common.remove(element)\n",
    "                break\n",
    "    \n",
    "    return common\n",
    "\n",
    "# Optimized version\n",
    "def find_common_elements_fast(lists: List[List[int]]) -> List[int]:\n",
    "    \"\"\"Find elements common to all lists (optimized version).\"\"\"\n",
    "    if not lists:\n",
    "        return []\n",
    "    \n",
    "    # Convert to sets for O(1) lookup\n",
    "    sets = [set(lst) for lst in lists]\n",
    "    \n",
    "    # Start with smallest set for efficiency\n",
    "    sets.sort(key=len)\n",
    "    common = sets[0].copy()\n",
    "    \n",
    "    # Intersect with remaining sets\n",
    "    for other_set in sets[1:]:\n",
    "        common &= other_set\n",
    "    \n",
    "    return sorted(list(common))\n",
    "\n",
    "# Test data\n",
    "import random\n",
    "test_lists = [\n",
    "    [random.randint(1, 1000) for _ in range(500)] for _ in range(5)\n",
    "]\n",
    "\n",
    "# Add some common elements\n",
    "common_elements = [42, 73, 99]\n",
    "for lst in test_lists:\n",
    "    lst.extend(common_elements)\n",
    "\n",
    "print(\"Optimizing Common Elements Search:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Test slow version (only for small data)\n",
    "small_test = [[1, 2, 3, 4, 5], [2, 3, 4, 5, 6], [3, 4, 5, 6, 7]]\n",
    "result1, time1 = measure_time(find_common_elements_slow, small_test)\n",
    "print(f\"Slow version: {time1:.6f}s (result: {result1})\")\n",
    "\n",
    "# Test fast version\n",
    "result2, time2 = measure_time(find_common_elements_fast, test_lists)\n",
    "print(f\"Fast version: {time2:.6f}s (found {len(result2)} common elements)\")\n",
    "\n",
    "# Verify results for small test\n",
    "result1_fast, _ = measure_time(find_common_elements_fast, small_test)\n",
    "assert result1 == result1_fast\n",
    "print(\"\\nâœ… Results are correct!\")\n",
    "\n",
    "if time1 > 0:\n",
    "    print(f\"Optimization gives {time1/time2:.1f}x speedup on large data!\")\n",
    "else:\n",
    "    print(\"Fast version handles large datasets efficiently!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook covered:\n",
    "- âœ… Performance measurement techniques\n",
    "- âœ… Algorithm optimization examples\n",
    "- âœ… Memory optimization strategies\n",
    "- âœ… Profiling and benchmarking\n",
    "- âœ… Common performance pitfalls\n",
    "- âœ… Caching and memoization\n",
    "- âœ… Performance best practices\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Measure First**: Always profile before optimizing\n",
    "2. **Algorithm Matters**: Choose the right algorithm for the job\n",
    "3. **Data Structures**: Use appropriate data structures for the task\n",
    "4. **Memory vs Speed**: Consider the trade-offs\n",
    "5. **Cache Wisely**: Use caching for expensive computations\n",
    "6. **Avoid Pitfalls**: Watch out for common performance anti-patterns\n",
    "7. **Test Thoroughly**: Ensure optimizations don't break functionality\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Profile your code** using the techniques learned\n",
    "2. **Identify bottlenecks** in your applications\n",
    "3. **Apply optimizations** systematically\n",
    "4. **Measure improvements** to ensure they're worthwhile\n",
    "5. **Learn advanced profiling** tools when available\n",
    "\n",
    "Remember: Premature optimization is the root of all evil. Focus on writing correct, maintainable code first, then optimize the bottlenecks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
