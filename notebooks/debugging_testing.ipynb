{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging and Testing in Python\n",
    "\n",
    "This notebook covers essential debugging techniques and testing practices for offline Python development. Learn how to find and fix bugs, write effective tests, and ensure code quality without internet access.\n",
    "\n",
    "## What you'll learn:\n",
    "- Debugging techniques and tools\n",
    "- Writing and running unit tests\n",
    "- Test-driven development (TDD)\n",
    "- Code coverage analysis\n",
    "- Common debugging patterns\n",
    "- Error handling best practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Debugging Techniques\n",
    "\n",
    "### Using Print Statements for Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Debugging a simple function\n",
    "def calculate_average(numbers):\n",
    "    print(f\"DEBUG: Input numbers: {numbers}\")\n",
    "    if not numbers:\n",
    "        print(\"DEBUG: Empty list detected\")\n",
    "        return 0\n",
    "    \n",
    "    total = sum(numbers)\n",
    "    print(f\"DEBUG: Sum of numbers: {total}\")\n",
    "    \n",
    "    average = total / len(numbers)\n",
    "    print(f\"DEBUG: Calculated average: {average}\")\n",
    "    \n",
    "    return average\n",
    "\n",
    "# Test the function\n",
    "result = calculate_average([1, 2, 3, 4, 5])\n",
    "print(f\"Final result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Assertions for Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_numbers(a, b):\n",
    "    assert b != 0, \"Division by zero is not allowed\"\n",
    "    assert isinstance(a, (int, float)), f\"First argument must be a number, got {type(a)}\"\n",
    "    assert isinstance(b, (int, float)), f\"Second argument must be a number, got {type(b)}\"\n",
    "    \n",
    "    return a / b\n",
    "\n",
    "# Test with valid inputs\n",
    "print(\"Valid division:\", divide_numbers(10, 2))\n",
    "\n",
    "# Test with invalid inputs (uncomment to see assertions)\n",
    "# print(\"Invalid division:\", divide_numbers(10, 0))\n",
    "# print(\"Invalid types:\", divide_numbers(\"10\", 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introduction to Unit Testing\n",
    "\n",
    "### Writing Your First Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple test without pytest\n",
    "def test_calculate_average():\n",
    "    # Test normal case\n",
    "    result = calculate_average([1, 2, 3, 4, 5])\n",
    "    expected = 3.0\n",
    "    assert result == expected, f\"Expected {expected}, got {result}\"\n",
    "    print(\"✅ Test passed: Normal case\")\n",
    "    \n",
    "    # Test empty list\n",
    "    result = calculate_average([])\n",
    "    expected = 0\n",
    "    assert result == expected, f\"Expected {expected}, got {result}\"\n",
    "    print(\"✅ Test passed: Empty list\")\n",
    "    \n",
    "    # Test single element\n",
    "    result = calculate_average([7])\n",
    "    expected = 7.0\n",
    "    assert result == expected, f\"Expected {expected}, got {result}\"\n",
    "    print(\"✅ Test passed: Single element\")\n",
    "\n",
    "# Run the tests\n",
    "test_calculate_average()\n",
    "print(\"All tests passed! 🎉\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test-Driven Development (TDD) Example\n",
    "\n",
    "Let's implement a function using TDD approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, write the test\n",
    "def test_fizzbuzz():\n",
    "    # Test multiples of 3\n",
    "    assert fizzbuzz(3) == \"Fizz\", f\"Expected 'Fizz', got {fizzbuzz(3)}\"\n",
    "    assert fizzbuzz(6) == \"Fizz\", f\"Expected 'Fizz', got {fizzbuzz(6)}\"\n",
    "    \n",
    "    # Test multiples of 5\n",
    "    assert fizzbuzz(5) == \"Buzz\", f\"Expected 'Buzz', got {fizzbuzz(5)}\"\n",
    "    assert fizzbuzz(10) == \"Buzz\", f\"Expected 'Buzz', got {fizzbuzz(10)}\"\n",
    "    \n",
    "    # Test multiples of both\n",
    "    assert fizzbuzz(15) == \"FizzBuzz\", f\"Expected 'FizzBuzz', got {fizzbuzz(15)}\"\n",
    "    assert fizzbuzz(30) == \"FizzBuzz\", f\"Expected 'FizzBuzz', got {fizzbuzz(30)}\"\n",
    "    \n",
    "    # Test non-multiples\n",
    "    assert fizzbuzz(1) == \"1\", f\"Expected '1', got {fizzbuzz(1)}\"\n",
    "    assert fizzbuzz(7) == \"7\", f\"Expected '7', got {fizzbuzz(7)}\"\n",
    "    \n",
    "    print(\"✅ All FizzBuzz tests passed!\")\n",
    "\n",
    "# Now implement the function\n",
    "def fizzbuzz(n):\n",
    "    if n % 3 == 0 and n % 5 == 0:\n",
    "        return \"FizzBuzz\"\n",
    "    elif n % 3 == 0:\n",
    "        return \"Fizz\"\n",
    "    elif n % 5 == 0:\n",
    "        return \"Buzz\"\n",
    "    else:\n",
    "        return str(n)\n",
    "\n",
    "# Run the tests\n",
    "test_fizzbuzz()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using pytest for Testing\n",
    "\n",
    "### Setting up pytest tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example pytest test file content (save this as test_example.py)\n",
    "pytest_test_content = '''\n",
    "import pytest\n",
    "from your_module import calculate_average, divide_numbers\n",
    "\n",
    "class TestCalculateAverage:\n",
    "    def test_normal_case(self):\n",
    "        assert calculate_average([1, 2, 3, 4, 5]) == 3.0\n",
    "    \n",
    "    def test_empty_list(self):\n",
    "        assert calculate_average([]) == 0\n",
    "    \n",
    "    def test_single_element(self):\n",
    "        assert calculate_average([7]) == 7.0\n",
    "    \n",
    "    def test_negative_numbers(self):\n",
    "        assert calculate_average([-1, -2, -3]) == -2.0\n",
    "\n",
    "class TestDivideNumbers:\n",
    "    def test_normal_division(self):\n",
    "        assert divide_numbers(10, 2) == 5.0\n",
    "    \n",
    "    def test_division_by_zero(self):\n",
    "        with pytest.raises(AssertionError):\n",
    "            divide_numbers(10, 0)\n",
    "    \n",
    "    def test_invalid_types(self):\n",
    "        with pytest.raises(AssertionError):\n",
    "            divide_numbers(\"10\", 2)\n",
    "'''\n",
    "\n",
    "print(\"Example pytest test file:\")\n",
    "print(pytest_test_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running pytest from command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commands to run pytest (run these in terminal)\n",
    "pytest_commands = [\n",
    "    \"pytest\",                           # Run all tests\n",
    "    \"pytest -v\",                        # Verbose output\n",
    "    \"pytest test_example.py\",           # Run specific test file\n",
    "    \"pytest -k 'test_normal'\",          # Run tests matching pattern\n",
    "    \"pytest --tb=short\",                # Shorter traceback\n",
    "    \"pytest -x\",                        # Stop on first failure\n",
    "    \"pytest --cov=your_module\",         # Code coverage (if pytest-cov installed)\n",
    "]\n",
    "\n",
    "print(\"Common pytest commands:\")\n",
    "for cmd in pytest_commands:\n",
    "    print(f\"  $ {cmd}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Debugging Common Errors\n",
    "\n",
    "### Syntax Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common syntax errors and how to fix them\n",
    "syntax_examples = {\n",
    "    \"Missing colon\": \"if x > 5\\nprint('Hello')\",  # Should be: if x > 5: print('Hello')\n",
    "    \"Incorrect indentation\": \"def func():\\n    print('Hello')\\n  print('World')\",  # Inconsistent indentation\n",
    "    \"Unmatched parentheses\": \"result = (1 + 2 * 3\",  # Missing closing parenthesis\n",
    "    \"Wrong quotes\": \"name = 'John\\\"\",  # Mixed quotes\n",
    "}\n",
    "\n",
    "for error_type, code in syntax_examples.items():\n",
    "    print(f\"{error_type}:\\n  {code}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtime Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common runtime errors\n",
    "def demonstrate_runtime_errors():\n",
    "    errors = []\n",
    "    \n",
    "    # IndexError\n",
    "    try:\n",
    "        my_list = [1, 2, 3]\n",
    "        print(my_list[5])\n",
    "    except IndexError as e:\n",
    "        errors.append(f\"IndexError: {e}\")\n",
    "    \n",
    "    # KeyError\n",
    "    try:\n",
    "        my_dict = {'a': 1, 'b': 2}\n",
    "        print(my_dict['c'])\n",
    "    except KeyError as e:\n",
    "        errors.append(f\"KeyError: {e}\")\n",
    "    \n",
    "    # TypeError\n",
    "    try:\n",
    "        result = \"hello\" + 5\n",
    "    except TypeError as e:\n",
    "        errors.append(f\"TypeError: {e}\")\n",
    "    \n",
    "    # ZeroDivisionError\n",
    "    try:\n",
    "        result = 10 / 0\n",
    "    except ZeroDivisionError as e:\n",
    "        errors.append(f\"ZeroDivisionError: {e}\")\n",
    "    \n",
    "    return errors\n",
    "\n",
    "runtime_errors = demonstrate_runtime_errors()\n",
    "print(\"Common runtime errors:\")\n",
    "for error in runtime_errors:\n",
    "    print(f\"  ❌ {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Code Quality Tools\n",
    "\n",
    "### Using Black for Code Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of unformatted code\n",
    "unformatted_code = '''\n",
    "def   calculate_total(items):\n",
    "    total=0\n",
    "    for item in items:\n",
    "        total+=item\n",
    "    return total\n",
    "'''\n",
    "\n",
    "# Black would format it as:\n",
    "formatted_code = '''\n",
    "def calculate_total(items):\n",
    "    total = 0\n",
    "    for item in items:\n",
    "        total += item\n",
    "    return total\n",
    "'''\n",
    "\n",
    "print(\"Before Black formatting:\")\n",
    "print(unformatted_code)\n",
    "print(\"\\nAfter Black formatting:\")\n",
    "print(formatted_code)\n",
    "\n",
    "# Commands to run Black\n",
    "print(\"\\nBlack commands:\")\n",
    "print(\"  $ black your_file.py          # Format a file\")\n",
    "print(\"  $ black .                     # Format all Python files in directory\")\n",
    "print(\"  $ black --check .             # Check if files need formatting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Ruff for Linting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruff commands for offline linting\n",
    "ruff_commands = [\n",
    "    \"ruff check your_file.py\",           # Check for linting errors\n",
    "    \"ruff check .\",                      # Check all files in directory\n",
    "    \"ruff check --fix .\",                # Auto-fix issues where possible\n",
    "    \"ruff format .\",                     # Format code (alternative to black)\n",
    "    \"ruff check --select F .\",           # Check only Pyflakes rules\n",
    "    \"ruff check --select E,W .\",         # Check only pycodestyle rules\n",
    "]\n",
    "\n",
    "print(\"Ruff commands for offline development:\")\n",
    "for cmd in ruff_commands:\n",
    "    print(f\"  $ {cmd}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Debugging Techniques\n",
    "\n",
    "### Using pdb (Python Debugger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using pdb\n",
    "pdb_example = '''\n",
    "import pdb\n",
    "\n",
    "def complex_function(x, y):\n",
    "    pdb.set_trace()  # Set breakpoint here\n",
    "    result = x * 2\n",
    "    result += y\n",
    "    return result\n",
    "\n",
    "# Run with: python -m pdb your_script.py\n",
    "# Or add pdb.set_trace() in your code\n",
    "'''\n",
    "\n",
    "print(\"Using pdb for debugging:\")\n",
    "print(pdb_example)\n",
    "\n",
    "print(\"Common pdb commands:\")\n",
    "pdb_commands = [\n",
    "    \"n (next)\",        # Execute next line\n",
    "    \"s (step)\",        # Step into function\n",
    "    \"c (continue)\",    # Continue execution\n",
    "    \"l (list)\",        # Show current code\n",
    "    \"p variable\",      # Print variable value\n",
    "    \"q (quit)\",        # Quit debugger\n",
    "]\n",
    "\n",
    "for cmd in pdb_commands:\n",
    "    print(f\"  {cmd}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Coverage\n",
    "\n",
    "### Understanding Code Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of calculating test coverage manually\n",
    "def analyze_test_coverage():\n",
    "    \"\"\"Analyze which lines of code are covered by tests.\"\"\"\n",
    "    \n",
    "    # Original function\n",
    "    def is_even(n):\n",
    "        if n % 2 == 0:      # Line 1\n",
    "            return True      # Line 2\n",
    "        else:                # Line 3\n",
    "            return False     # Line 4\n",
    "    \n",
    "    # Test cases\n",
    "    test_cases = [\n",
    "        (2, True),   # Covers lines 1, 2\n",
    "        (3, False),  # Covers lines 1, 3, 4\n",
    "        (0, True),   # Covers lines 1, 2\n",
    "        (-2, True),  # Covers lines 1, 2\n",
    "    ]\n",
    "    \n",
    "    covered_lines = set()\n",
    "    \n",
    "    for input_val, expected in test_cases:\n",
    "        result = is_even(input_val)\n",
    "        assert result == expected\n",
    "        \n",
    "        # Track coverage\n",
    "        if input_val % 2 == 0:\n",
    "            covered_lines.update([1, 2])\n",
    "        else:\n",
    "            covered_lines.update([1, 3, 4])\n",
    "    \n",
    "    total_lines = 4\n",
    "    coverage_percentage = (len(covered_lines) / total_lines) * 100\n",
    "    \n",
    "    print(f\"Lines covered: {sorted(covered_lines)}\")\n",
    "    print(f\"Total lines: {total_lines}\")\n",
    "    print(f\"Coverage: {coverage_percentage}%\")\n",
    "    \n",
    "    return coverage_percentage\n",
    "\n",
    "analyze_test_coverage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Best Practices for Testing and Debugging\n",
    "\n",
    "### Testing Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_best_practices = {\n",
    "    \"Test Structure\": [\n",
    "        \"Test one thing per test function\",\n",
    "        \"Use descriptive test names\",\n",
    "        \"Group related tests in classes\",\n",
    "        \"Test edge cases and error conditions\"\n",
    "    ],\n",
    "    \"Test Types\": [\n",
    "        \"Unit tests for individual functions\",\n",
    "        \"Integration tests for component interaction\",\n",
    "        \"Regression tests for bug fixes\",\n",
    "        \"Performance tests for critical paths\"\n",
    "    ],\n",
    "    \"Debugging Tips\": [\n",
    "        \"Reproduce the bug consistently\",\n",
    "        \"Use print statements strategically\",\n",
    "        \"Check variable values at key points\",\n",
    "        \"Isolate the problematic code section\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, practices in testing_best_practices.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for practice in practices:\n",
    "        print(f\"  ✅ {practice}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Exercise: Debug and Test a Function\n",
    "\n",
    "Here's a buggy function. Your task is to:\n",
    "1. Find and fix the bugs\n",
    "2. Write comprehensive tests\n",
    "3. Ensure good test coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buggy function to debug and test\n",
    "def find_max_in_list(numbers):\n",
    "    \"\"\"Find the maximum value in a list of numbers.\"\"\"\n",
    "    if not numbers:\n",
    "        return None\n",
    "    \n",
    "    max_val = numbers[0]\n",
    "    for num in numbers:\n",
    "        if num > max_val:\n",
    "            max_val = num\n",
    "    \n",
    "    return max_val\n",
    "\n",
    "# Test cases to try\n",
    "test_cases = [\n",
    "    ([1, 5, 3, 9, 2], 9),\n",
    "    ([], None),\n",
    "    ([7], 7),\n",
    "    ([-1, -5, -3], -1),\n",
    "    ([1, 1, 1], 1),\n",
    "]\n",
    "\n",
    "print(\"Testing the function:\")\n",
    "for input_list, expected in test_cases:\n",
    "    result = find_max_in_list(input_list)\n",
    "    status = \"✅\" if result == expected else \"❌\"\n",
    "    print(f\"  {status} find_max_in_list({input_list}) = {result} (expected {expected})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Running Tests Offline\n",
    "\n",
    "### Setting up a local test runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple test runner script\n",
    "test_runner_script = '''\n",
    "# test_runner.py\n",
    "import os\n",
    "import importlib.util\n",
    "import traceback\n",
    "\n",
    "def run_tests(test_dir):\n",
    "    \"\"\"Run all test files in a directory.\"\"\"\n",
    "    passed = 0\n",
    "    failed = 0\n",
    "    \n",
    "    for filename in os.listdir(test_dir):\n",
    "        if filename.startswith('test_') and filename.endswith('.py'):\n",
    "            print(f\"\\nRunning {filename}...\")\n",
    "            \n",
    "            try:\n",
    "                # Import the test module\n",
    "                spec = importlib.util.spec_from_file_location(\n",
    "                    filename[:-3], os.path.join(test_dir, filename)\n",
    "                )\n",
    "                module = importlib.util.module_from_spec(spec)\n",
    "                spec.loader.exec_module(module)\n",
    "                \n",
    "                # Run test functions\n",
    "                for attr_name in dir(module):\n",
    "                    if attr_name.startswith('test_'):\n",
    "                        test_func = getattr(module, attr_name)\n",
    "                        try:\n",
    "                            test_func()\n",
    "                            print(f\"  ✅ {attr_name}\")\n",
    "                            passed += 1\n",
    "                        except Exception as e:\n",
    "                            print(f\"  ❌ {attr_name}: {e}\")\n",
    "                            failed += 1\n",
    "                            \n",
    "            except Exception as e:\n",
    "                print(f\"  ❌ Error loading {filename}: {e}\")\n",
    "                failed += 1\n",
    "    \n",
    "    print(f\"\\nResults: {passed} passed, {failed} failed\")\n",
    "    return passed, failed\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_tests('.')\n",
    "'''\n",
    "\n",
    "print(\"Simple test runner script:\")\n",
    "print(test_runner_script[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook covered:\n",
    "- ✅ Basic debugging with print statements and assertions\n",
    "- ✅ Writing and running unit tests\n",
    "- ✅ Test-driven development approach\n",
    "- ✅ Using pytest for professional testing\n",
    "- ✅ Common error types and how to fix them\n",
    "- ✅ Code quality tools (Black, Ruff)\n",
    "- ✅ Advanced debugging with pdb\n",
    "- ✅ Test coverage concepts\n",
    "- ✅ Best practices for testing and debugging\n",
    "- ✅ Offline test running capabilities\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Practice debugging** real code with bugs\n",
    "2. **Write tests** for your existing functions\n",
    "3. **Set up pytest** in your development environment\n",
    "4. **Learn Black and Ruff** for code quality\n",
    "5. **Master pdb** for complex debugging scenarios\n",
    "\n",
    "Remember: Good testing and debugging skills are essential for reliable software development, especially in offline environments where you can't rely on external tools or services."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
